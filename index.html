<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>ZHANG Yuechen</title>
  <meta name="author" content="ZHANG Yuechen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet_new.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Kalam:wght@400;700&display=swap" rel="stylesheet">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü••</text></svg>">
</head>

<body>
  <!-- Âø´ÈÄüÂØºËà™Ê†è -->
  <nav class="fast-nav">
    <div class="nav-container">
      <div class="nav-title">YUECHEN ZHANG's WEBPAGE</div>
      <div class="nav-links">
        <a href="#about" class="nav-link">About</a>
        <a href="#representative" class="nav-link">Highlights</a>
        <a href="#publications" class="nav-link">Publications</a>
        <a href="#education" class="nav-link">Experience</a>
        <a href="#awards" class="nav-link">Awards</a>
      </div>
    </div>
  </nav>


  <!-- Â§¥ÈÉ®Âå∫Âüü -->
  <div id="about" class="header-container">
    <span class="header-motion-hint header-motion-hint-top">Double-click to toggle motion / ÂèåÂáªÂàáÊç¢Âä®Êïà</span>
    <div class="header-motion-controls">
      <button class="header-motion-toggle" type="button" aria-pressed="true">Motion On</button>
    </div>
    <div class="header-pattern header-pattern-base" aria-hidden="true"></div>
    <div class="header-content-stack">
      <div class="header-content header-content-base">
        <div class="header-text">
          <h1 class="header-title">ZHANG Yuechen</h1>
          <p class="header-description">
            Hi! I am a Research Scientist in Multi-Modal at <a href="https://mimo.xiaomi.com/">Xiaomi MiMo</a>.
            <br><br>
            I obtained my PhD from <a href="https://www.cuhk.edu.hk/english/index.html">CUHK</a> in 2025, advised by <a href="https://jiaya.me">Prof. Jiaya Jia</a>.
            Before that, I received my bachelor's degree from <a href="https://www.cuhk.edu.hk/english/index.html">CUHK</a> in 2021.
            I focus on <strong>autoregressive, interactive, and efficient video generation and intelligence</strong>.
            I also enjoy exploring <strong>special and interesting visual effects and applications</strong> in generation tasks.
          </p>
          <div class="header-links">
            <a href="mailto:zhangyc@link.cuhk.edu.hk" class="header-link">Email</a>
            <a href="https://scholar.google.com/citations?user=8OijNgkAAAAJ&hl" class="header-link">Google Scholar</a>
            <a href="https://github.com/JulianJuaner" class="header-link">Github</a>
          </div>
        </div>
        <div class="header-image" aria-hidden="true">
          <br>
          <img src="images/IMG_2755_2_dark.png" alt="">
          <br>
          <img src="images/interests_2_dark.png" alt="">
        </div>
      </div>
    </div>
    <div class="header-mask-layer" aria-hidden="true">
      <div class="header-pattern"></div>
      <div class="header-content header-content-masked">
        <div class="header-text">
          <h1 class="header-title"><strong>Âº†Â≤≥Êô®</strong></h1>
          <p class="header-description">
            ‰Ω†Â•ΩÔºÅÊàëÁõÆÂâçÊòØ <a href="https://mimo.xiaomi.com/">Â∞èÁ±≥ MiMo (LLM-Core)</a> ÁöÑÂ§öÊ®°ÊÄÅÁÆóÊ≥ïÁ†îÁ©∂Âëò„ÄÇ
            <br><br>
            Êàë‰∫é 2025 Âπ¥ÂçöÂ£´ÊØï‰∏ö‰∫é <a href="https://www.cuhk.edu.hk/english/index.html">È¶ôÊ∏Ø‰∏≠ÊñáÂ§ßÂ≠¶</a>ÔºåÂØºÂ∏à‰∏∫ <a href="https://jiaya.me">Ë¥æ‰Ω≥‰∫öÊïôÊéà</a>„ÄÇ
            Âú®Ê≠§‰πãÂâçÔºåÊàë‰∫é 2021 Âπ¥Ëé∑Âæó <a href="https://www.cuhk.edu.hk/english/index.html">È¶ôÊ∏Ø‰∏≠ÊñáÂ§ßÂ≠¶</a> ÁöÑËÆ°ÁÆóÊú∫ÁßëÂ≠¶Â≠¶Â£´Â≠¶‰Ωç„ÄÇ<br>
            ÊàëÁöÑÁ†îÁ©∂ÊñπÂêëËÅöÁÑ¶ <strong>Ëá™ÂõûÂΩí„ÄÅ‰∫§‰∫íÂºè„ÄÅÈ´òÊïàÁöÑËßÜÈ¢ëÁîüÊàê‰∏éÊô∫ËÉΩ</strong>„ÄÇ<br>
            Êàë‰πüÂñúÊ¨¢Êé¢Á¥¢ÁîüÊàê‰ªªÂä°‰∏≠ÁöÑ <strong>ÁâπÊÆä‰∏îÊúâË∂£ÁöÑËßÜËßâÊïàÊûú‰∏éÂ∫îÁî®</strong>„ÄÇ
          </p>
          <div class="header-links">
            <a href="mailto:zhangyc@link.cuhk.edu.hk" class="header-link">ÈÇÆÁÆ±</a>
            <a href="https://scholar.google.com/citations?user=8OijNgkAAAAJ&hl" class="header-link">Ë∞∑Ê≠åÂ≠¶ÊúØ</a>
            <a href="https://github.com/JulianJuaner" class="header-link">‰ª£Á†Å</a>
          </div>
        </div>
        <div class="header-image">
          <br>
          <img src="images/IMG_2755_2.png" alt="ZHANG Yuechen">
          <br>
          <img src="images/interests_2.png" alt="research interests">
        </div>
      </div>
    </div>
    </div>
    <span class="header-motion-hint header-motion-hint-bottom">Double-click to toggle motion / ÂèåÂáªÂàáÊç¢Âä®Êïà</span>
    <!-- <div class="header-research-interests">
      <p class="research-interests-text">
        I am interested in <strong>autoregressive, interactive, and efficient video generation and intelligence</strong>. <br>
        I am also interested in <strong>special and interesting visual effects and applications</strong> in generation tasks
      </p>
    </div> -->
    <!-- <div class="header-bottom-image">
      <img class="image-center image-landscape" src="image.png" alt="ZHANG Yuechen">
      <img class="image-center image-portrait" src="image_ver.png" alt="ZHANG Yuechen">
    </div> -->
  </div>
  <!-- Representative Works ÈÉ®ÂàÜ -->
  <section id="representative" class="works-section">
    <div class="section-header section-header--dark">
      <span class="section-sub">HOVER IMAGES TO SHOW RESULTS</span>
      <h2 class="section-title">Representative Research Works</h2>
    </div>
    <br><br>
    <div class="works-carousel">
      <div class="works-carousel-stage">
        <div class="works-carousel-track" aria-label="Representative works carousel">
        <!-- Jenga -->
        <article class="works-card" data-index="0" data-abstract="Despite the remarkable generation quality of video Diffusion Transformer (DiT) models, their practical deployment is severely hindered by extensive computational requirements. This inefficiency stems from two key challenges: the quadratic complexity of self-attention with respect to token length and the multi-step nature of diffusion models. To address these limitations, we present Jenga, a novel inference pipeline that combines dynamic attention carving with progressive resolution generation. Our approach leverages two key insights: (1) early denoising steps do not require high-resolution latents, and (2) later steps do not require dense attention. Jenga introduces a block-wise attention mechanism that dynamically selects relevant token interactions using 3D space-filling curves, alongside a progressive resolution strategy that gradually increases latent resolution during generation. Experimental results demonstrate that Jenga achieves substantial speedups across multiple state-of-the-art video diffusion models while maintaining comparable generation quality (8.83√ó speedup with 0.01% performance drop on VBench). As a plug-and-play solution, Jenga enables practical, high-quality video generation on modern hardware by reducing inference time from minutes to seconds‚Äîwithout requiring model retraining.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/jenga.png" alt="Jenga" class="bigcard-img img-default" />
              <img src="images/jenga_default.gif" alt="Jenga" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">Training-Free Efficient Video Generation via Dynamic Token Carving</div>
                <div class="bigcard-authors"><span class="author-name">Yuechen Zhang</span>, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, Jiaya Jia</div>
                <div class="bigcard-venue">NeurIPS, 2025</div>
                <div class="bigcard-desc">Jenga accelerates HunyuanVideo by 4.68-10.35√ó through dynamic attention carving and progressive resolution generation.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2505.16864" class="bigcard-link">arXiv</a>
                <a href="projects/jenga/index.html" class="bigcard-link">Project</a>
                <a href="https://github.com/JIA-Lab-research/Jenga" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>

        <!-- MagicMirror -->
        <article class="works-card" data-index="1" data-abstract="We present MagicMirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that MagicMirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/mg.png" alt="MagicMirror" class="bigcard-img img-default" />
              <img src="images/magicmirror-default.gif" alt="MagicMirror" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers</div>
                <div class="bigcard-authors"><span class="author-name">Yuechen Zhang*</span>, Yaoyang Liu*, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, Jiaya Jia</div>
                <div class="bigcard-venue">ICCV, 2025</div>
                <div class="bigcard-desc">MagicMirror generates identity-preserved videos from reference images using a conditional adaptive normalization module.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2501.03931" class="bigcard-link">arXiv</a>
                <a href="projects/MagicMirror/index.html" class="bigcard-link">Project</a>
                <a href="https://github.com/JIA-Lab-research/MagicMirror" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>

        <!-- Mini-Gemini -->
        <article class="works-card" data-index="2" data-abstract="In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance across various cross-modal tasks from three aspects: high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and generation simultaneously. Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs) from 2B to 34B. It is demonstrated to achieve leading performance in several zero-shot benchmarks and even surpasses the developed private models.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/mgm.png" alt="Mini-Gemini" class="bigcard-img img-default" />
              <img src="images/minigemini.png" alt="Mini-Gemini" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</div>
                <div class="bigcard-authors">Yanwei Li*, <span class="author-name">Yuechen Zhang*</span>, Chengyao Wang*, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia</div>
                <div class="bigcard-venue">TPAMI, 2025</div>
                <div class="bigcard-desc">Mini-Gemini is a novel framework ranges from 2B to 34B VLMs for hi-resolution image understanding and reasoning-aware image generation.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2403.18814" class="bigcard-link">arXiv</a>
                <a href="https://mini-gemini.github.io/" class="bigcard-link">Project</a>
                <a href="https://github.com/JIA-Lab-research/MiniGemini" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>

        <!-- Prompt Highlighter -->
        <article class="works-card" data-index="3" data-abstract="This study targets a critical aspect of multi-modal LLMs' inference: explicit controllable text generation. Multi-modal LLMs empower multi-modality understanding with the capability of semantic generation yet bring less explainability and heavier reliance on prompt contents due to their autoregressive generative nature. While manipulating prompt formats could improve outputs, designing specific and precise prompts per task can be challenging and ineffective. To tackle this issue we introduce a novel inference method Prompt Highlighter which enables users to highlight specific prompt spans to interactively control the focus during generation. Motivated by the classifier-free diffusion guidance we form regular and unconditional context pairs based on highlighted tokens, demonstrating that the autoregressive generation in models can be guided in a classifier-free way. Notably we find that during inference guiding the models with highlighted tokens through the attention weights leads to more desired outputs. Our approach is compatible with current LLMs and VLMs, achieving impressive customized generation results without training. Experiments confirm its effectiveness in focusing on input contexts and generating reliable content. Without tuning on LLaVA-v1.5 our method secured 70.7 in the MMBench test and 1552.5 in MME-perception.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/ph.png" alt="Prompt Highlighter" class="bigcard-img img-default" />
              <img src="images/ph.png" alt="Prompt Highlighter" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</div>
                <div class="bigcard-authors"><span class="author-name">Yuechen Zhang</span>, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia</div>
                <div class="bigcard-venue">CVPR, 2024</div>
                <div class="bigcard-desc">Prompt Highlighter is a training-free inference pipeline, which facilitates token-level user interactions for customized generation.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2312.04302" class="bigcard-link">arXiv</a>
                <a href="projects/PromptHighlighter/" class="bigcard-link">Project</a>
                <a href="https://github.com/JIA-Lab-research/Prompt-Highlighter/" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>

        <!-- RIVAL -->
        <article class="works-card" data-index="4" data-abstract="Recent diffusion model advancements have enabled high-fidelity images to be generated using text prompts. However, a domain gap exists between generated images and real-world images, which poses a challenge in generating high-quality variations of real-world images. Our investigation uncovers that this domain gap originates from a latent distribution gap in different diffusion processes. To address this issue, we propose a novel inference pipeline called Real-world Image Variation by ALignment (RIVAL) that utilizes diffusion models to generate image variations from a single image exemplar. Our pipeline enhances the generation quality of image variations by aligning the image generation process to the source image's inversion chain. Specifically, we demonstrate that step-wise latent distribution alignment is essential for generating high-quality variations. To attain this, we design a cross-image self-attention injection for feature interaction and a step-wise distribution normalization to align the latent features. Incorporating these alignment processes into a diffusion model allows RIVAL to generate high-quality image variations without further parameter optimization. Our experimental results demonstrate that our proposed approach outperforms existing methods concerning semantic similarity and perceptual quality. This generalized inference pipeline can be easily applied to other diffusion-based generation tasks, such as image-conditioned text-to-image generation and stylization.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/rival.png" alt="RIVAL" class="bigcard-img img-default" />
              <img src="images/rival-default.png" alt="RIVAL" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">Real-World Image Variation by Aligning Diffusion Inversion Chain</div>
                <div class="bigcard-authors"><span class="author-name">Yuechen Zhang</span>, Jinbo Xing, Eric Lo, Jiaya Jia</div>
                <div class="bigcard-venue">NeurIPS (Spotlight), 2023</div>
                <div class="bigcard-desc">Given an image as the prompt, we can generate its variations by aligning the diffusion inversion chain. The variations are diverse and controllable.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2305.18729" class="bigcard-link">arXiv</a>
                <a href="https://rival-diff.github.io/" class="bigcard-link">Project</a>
                <a href="https://github.com/julianjuaner/RIVAL/" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>

        <!-- Ref-NPR -->
        <article class="works-card" data-index="5" data-abstract="Current 3D scene stylization methods transfer textures and colors as styles using arbitrary style references, lacking meaningful semantic correspondences. We introduce Reference-Based Non-Photorealistic Radiance Fields (Ref-NPR) to address this limitation. This controllable method stylizes a 3D scene using radiance fields with a single stylized 2D view as a reference. We propose a ray registration process based on the stylized reference view to obtain pseudo-ray supervision in novel views. Then we exploit semantic correspondences in content images to fill occluded regions with perceptually similar styles, resulting in non-photorealistic and continuous novel view sequences. Our experimental results demonstrate that Ref-NPR outperforms existing scene and video stylization methods regarding visual quality and semantic correspondence.">
          <div class="works-card-inner">
            <div class="bigcard-img-wrap hover-img-box">
              <img src="posters/refnpr.png" alt="Ref-NPR" class="bigcard-img img-default" />
              <img src="images/ref-npr-default.gif" alt="Ref-NPR" class="bigcard-img img-hover" />
            </div>
            <div class="bigcard-content">
              <div>
                <div class="bigcard-title">Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields</div>
                <div class="bigcard-authors"><span class="author-name">Yuechen Zhang</span>, Zexin He, Jinbo Xing, Xufeng Yao, Jiaya Jia</div>
                <div class="bigcard-venue">CVPR, 2023</div>
                <div class="bigcard-desc">We present a controllable scene stylization method utilizing radiance fields to stylize a 3D scene, with a single stylized 2D view taken as reference.</div>
              </div>
              <div class="bigcard-links">
                <a href="https://arxiv.org/abs/2212.02766" class="bigcard-link">arXiv</a>
                <a href="https://ref-npr.github.io" class="bigcard-link">Project</a>
                <a href="https://github.com/JIA-Lab-research/Ref-NPR" class="bigcard-link">Code</a>
              </div>
            </div>
          </div>
          <div class="works-card-abstract" aria-hidden="true"></div>
        </article>
        </div>
      </div>
      <button type="button" class="works-carousel-prev" aria-label="Previous">‚Äπ</button>
      <button type="button" class="works-carousel-next" aria-label="Next">‚Ä∫</button>
      <div class="works-carousel-nav">
        <div class="works-carousel-dots" role="tablist" aria-label="Slide index"></div>
      </div>
    </div>
  </section>

  <!-- Full Publications ÈÉ®ÂàÜ -->
  <section id="publications" class="publications-section">
    <div class="section-header">
      <span class="section-sub">FULL PUBLICATIONS</span>
      <h2 class="section-title">Full Publications</h2>
    </div>
    <div class="publications-list">
      <div class="publications-category">
        <h3 class="publications-category-title">AIGC</h3>

      <!-- UnityVideo (CVPR 2026) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/unityvideo.png" alt="unityvideo" class="pub-img-default" />
          <img src="images/unityvideo-default.png" alt="unityvideo" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">UnityVideo : Unified Multi-Modal Multi-Task Learning for Enhancing World-Aware Video Generation</div>
          <div class="publication-authors">Jiehui Huang, <span class="author-name">Yuechen Zhang</span>, Xu He, Yuan Gao, Zhi Cen, Bin Xia, Yan Zhou,  Xin Tao, Pengfei Wan, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2026</div>
          <div class="publication-links">
            <a href="https://jackailab.github.io/Projects/UnityVideo/">Project</a> /
            <a href="https://huggingface.co/datasets/JackAILab/OpenUni">Data</a> /
            <a href="https://arxiv.org/abs/2512.07831">arXiv</a> /
            <a href="https://github.com/JIA-Lab-research/UnityVideo" class="code-link" data-github-repo="JIA-Lab-research/UnityVideo">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/UnityVideo"></span>
          </div>
        </div>
      </div>

      <!-- DreamOmni2 (CVPR 2026) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/do2_default.png" alt="DreamOmni2" class="pub-img-default" />
          <img src="images/do2.png" alt="DreamOmni2" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">DreamOmni2: Multimodal Instruction-based Editing and Generation</div>
          <div class="publication-authors">Bin Xia, Bohao Peng, <span class="author-name">Yuechen Zhang</span>, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2026</div>
          <div class="publication-links">
            <a href="https://pbihao.github.io/projects/DreamOmni2/index.html">Project</a> /
            <a href="https://huggingface.co/spaces/wcy1122/DreamOmni2-Gen">Demo</a> /
            <a href="https://arxiv.org/abs/2510.06679">arXiv</a> /
            <a href="https://github.com/JIA-Lab-research/DreamOmni2" class="code-link" data-github-repo="JIA-Lab-research/DreamOmni2">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/DreamOmni2"></span>
          </div>
        </div>
      </div>


      <!-- Jenga -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/jenga_default.gif" alt="Jenga" class="pub-img-default" />
          <img src="images/jenga.png" alt="Jenga" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Training-Free Efficient Video Generation via Dynamic Token Carving</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang</span>, Jinbo Xing, Bin Xia, Shaoteng Liu, Bohao Peng, Xin Tao, Pengfei Wan, Eric Lo, Jiaya Jia</div>
          <div class="publication-venue">NeurIPS, 2025</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2505.16864">arXiv</a> /
            <a href="projects/jenga/index.html">Project</a> /
            <a href="https://github.com/JIA-Lab-research/Jenga" class="code-link" data-github-repo="JIA-Lab-research/Jenga">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/Jenga"></span>
          </div>
        </div>
      </div>

      <!-- MagicMirror -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/magicmirror-default.gif" alt="MagicMirror" class="pub-img-default" />
          <img src="images/magicmirror.png" alt="MagicMirror" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">MagicMirror: ID-Preserved Video Generation in Video Diffusion Transformers</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang*</span>, Yaoyang Liu*, Bin Xia, Bohao Peng, Zexin Yan, Eric Lo, Jiaya Jia</div>
          <div class="publication-venue">ICCV, 2025</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2501.03931">arXiv</a> /
            <a href="projects/MagicMirror/index.html">Project</a> /
            <a href="https://github.com/JIA-Lab-research/MagicMirror" class="code-link" data-github-repo="JIA-Lab-research/MagicMirror">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/MagicMirror"></span>
          </div>
        </div>
      </div>

      <!-- DreamOmni (CVPR 2025) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/dreamomni-default.png" alt="DreamOmni" class="pub-img-default" />
          <img src="images/dreamomni.png" alt="DreamOmni" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">DreamOmni: Unified Image Generation and Editing</div>
          <div class="publication-authors">Bin Xia, <span class="author-name">Yuechen Zhang</span>, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2025</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2406.02816">arXiv</a> /
            <a href="https://zj-binxia.github.io/DreamOmni-ProjectPage/">Project</a> /
            <a href="https://github.com/zj-binxia/DreamOmni" class="code-link" data-github-repo="zj-binxia/DreamOmni">Code</a><span class="github-stars" data-github-repo="zj-binxia/DreamOmni"></span>
          </div>
        </div>
      </div>

      <!-- ResMaster (AAAI 2025) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/resmaster.png" alt="ResMaster" class="pub-img-default" />
          <img src="images/resmaster.png" alt="ResMaster" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">ResMaster: Mastering High-Resolution Image Generation via Structural and Fine-Grained Guidance</div>
          <div class="publication-authors">Shuwei Shi, Wenbo Li, <span class="author-name">Yuechen Zhang</span>, Jingwen He, Biao Gong, Yinqiang Zheng</div>
          <div class="publication-venue">AAAI, 2025</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2406.16476">arXiv</a>
          </div>
        </div>
      </div>

      <!-- Make-Your-Video (TVCG 2025) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/make-your-video.gif" alt="Make-Your-Video" class="pub-img-default" />
          <img src="images/make-your-video.gif" alt="Make-Your-Video" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance</div>
          <div class="publication-authors">Jinbo Xing, Menghan Xia, Yuxin Liu, <span class="author-name">Yuechen Zhang</span>, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, Tien-Tsin Wong</div>
          <div class="publication-venue">TVCG, 2025</div>
          <div class="publication-links">
            <a href="https://www.computer.org/csdl/journal/tg/2025/02/10436391/1UwVf2MBnby">Paper</a> /
            <a href="https://arxiv.org/abs/2306.00943">arXiv</a> /
            <a href="https://doubiiu.github.io/projects/Make-Your-Video/">Project</a> /
            <a href="https://github.com/AILab-CVC/Make-Your-Video" class="code-link" data-github-repo="AILab-CVC/Make-Your-Video">Code</a><span class="github-stars" data-github-repo="AILab-CVC/Make-Your-Video"></span>
          </div>
          <div class="publication-desc">Customized video generation using textual and structural guidance.</div>
        </div>
      </div>

      <!-- Video-P2P (CVPR 2024) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/videop2p.gif" alt="Video-P2P" class="pub-img-default" />
          <img src="images/videop2p.png" alt="Video-P2P" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Video-P2P: Video Editing with Cross-attention Control</div>
          <div class="publication-authors">Shaoteng Liu, <span class="author-name">Yuechen Zhang</span>, Wenbo Li, Zhe Lin, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2024</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2303.04761">arXiv</a> /
            <a href="https://video-p2p.github.io/">Project</a> /
            <a href="https://github.com/ShaoTengLiu/Video-P2P" class="code-link" data-github-repo="ShaoTengLiu/Video-P2P">Code</a><span class="github-stars" data-github-repo="ShaoTengLiu/Video-P2P"></span>
          </div>
        </div>
      </div>

      <!-- ControlNeXt (Preprint 2024) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/controlnext-default.gif" alt="ControlNeXt" class="pub-img-default" />
          <img src="images/controlnext.png" alt="ControlNeXt" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">ControlNeXt: Powerful and Efficient Control for Image and Video Generation</div>
          <div class="publication-authors">Bohao Peng, Jian Wang, <span class="author-name">Yuechen Zhang</span>, Wenbo Li, Ming-Chang Yang, Jiaya Jia</div>
          <div class="publication-venue">Preprint, 2024</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2408.06070">arXiv</a> /
            <a href="https://pbihao.github.io/projects/controlnext/index.html">Project</a> /
            <a href="https://huggingface.co/spaces/Eugeoter/ControlNeXt">Demo</a> /
            <a href="https://github.com/JIA-Lab-research/ControlNeXt" class="code-link" data-github-repo="JIA-Lab-research/ControlNeXt">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/ControlNeXt"></span>
          </div>
        </div>
      </div>

      <!-- RIVAL (NeurIPS 2023) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/rival-default.png" alt="RIVAL" class="pub-img-default" />
          <img src="images/rival.png" alt="RIVAL" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Real-World Image Variation by Aligning Diffusion Inversion Chain</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang</span>, Jinbo Xing, Eric Lo, Jiaya Jia</div>
          <div class="publication-venue">NeurIPS (Spotlight), 2023</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2305.18729">arXiv</a> /
            <a href="https://rival-diff.github.io/">Project</a> /
            <a href="https://github.com/julianjuaner/RIVAL/" class="code-link" data-github-repo="julianjuaner/RIVAL">Code</a><span class="github-stars" data-github-repo="julianjuaner/RIVAL"></span>
          </div>
        </div>
      </div>

      <!-- Ref-NPR (CVPR 2023) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/ref-npr-default.gif" alt="Ref-NPR" class="pub-img-default" />
          <img src="images/ref-npr.png" alt="Ref-NPR" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Ref-NPR: Reference-Based Non-Photorealistic Radiance Fields</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang</span>, Zexin He, Jinbo Xing, Xufeng Yao, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2023</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2212.02766">arXiv</a> /
            <a href="https://ref-npr.github.io">Project</a> /
            <a href="https://github.com/JIA-Lab-research/Ref-NPR" class="code-link" data-github-repo="JIA-Lab-research/Ref-NPR">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/Ref-NPR"></span>
          </div>
        </div>
      </div>

      <!-- Flow-aware Synthesis (CVM 2021) -->
      <div class="publication-item">
        <div class="publication-image">
          <video class="pub-img-hover" muted autoplay loop>
            <source src="images/FVI.mp4" type="video/mp4">
          </video>
          <video class="pub-img-default" muted autoplay loop>
            <source src="images/FVI.mp4" type="video/mp4">
          </video>
        </div>
        <div class="publication-content">
          <div class="publication-title">Flow-aware Synthesis: A Generic Motion Model for Video Frame Interpolation</div>
          <div class="publication-authors">Jinbo Xing*, Wenbo Hu*, <span class="author-name">Yuechen Zhang</span>, Tien-Tsin Wong</div>
          <div class="publication-venue">Computational Visual Media (CVM), 2021</div>
          <div class="publication-links">
            <a href="https://link.springer.com/article/10.1007/s41095-021-0208-x">Paper</a>
          </div>
        </div>
      </div>

      </div>
      <div class="publications-category">
        <h3 class="publications-category-title">Controllable Large Language Models</h3>
      <!-- Lyra -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/lyra-default.png" alt="Lyra" class="pub-img-default" />
          <img src="images/lyra.png" alt="Lyra" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition</div>
          <div class="publication-authors">Zhisheng Zhong*, Chengyao Wang*, Yuqi Liu*, Senqiao Yang, Longxiang Tang, <span class="author-name">Yuechen Zhang</span>, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, Shaozuo Yu, Sitong Wu, Eric Lo, Shu Liu, Jiaya Jia</div>
          <div class="publication-venue">ICCV, 2025</div>
          <div class="publication-links">
            <a href="https://lyra-omni.github.io/">Project</a>
            <a href="https://arxiv.org/abs/2412.09501">arXiv</a> /
            <a href="https://www.youtube.com/watch?v=7kh-M0jmmtI">Video</a> /
            <a href="https://github.com/JIA-Lab-research/Lyra" class="code-link" data-github-repo="JIA-Lab-research/Lyra">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/Lyra"></span> /
          </div>
        </div>
      </div>

      <!-- Mini-Gemini -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/minigemini-default.png" alt="Mini-Gemini" class="pub-img-default" />
          <img src="images/minigemini.png" alt="Mini-Gemini" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models</div>
          <div class="publication-authors">Yanwei Li*, <span class="author-name">Yuechen Zhang*</span>, Chengyao Wang*, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia</div>
          <div class="publication-venue">TPAMI, 2025</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2403.18814">arXiv</a> /
            <a href="https://mini-gemini.github.io/">Project</a> /
            <a href="https://github.com/JIA-Lab-research/MiniGemini" class="code-link" data-github-repo="JIA-Lab-research/MiniGemini">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/MiniGemini"></span>
          </div>
        </div>
      </div>

      <!-- Prompt Highlighter -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/ph-default.png" alt="Prompt Highlighter" class="pub-img-default" />
          <img src="images/ph.png" alt="Prompt Highlighter" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang</span>, Shengju Qian, Bohao Peng, Shu Liu, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2024</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2312.04302">arXiv</a> /
            <a href="projects/PromptHighlighter/">Project</a> /
            <a href="https://github.com/JIA-Lab-research/Prompt-Highlighter/" class="code-link" data-github-repo="JIA-Lab-research/Prompt-Highlighter">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/Prompt-Highlighter"></span>
          </div>
        </div>
      </div>

    </div>
      <div class="publications-category">
        <h3 class="publications-category-title">Others</h3>
      <!-- UniVA (Preprint 2025) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/univa-default.png" alt="UniVA" class="pub-img-default" />
          <img src="images/univa-default.png" alt="UniVA" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">UniVA: Universal Video Agent towards Open-Source Next-Generation Video Generalist</div>
          <div class="publication-authors">Zhengyang Liang, Daoan Zhang, Huichi Zhou, Rui Huang, Bobo Li, <span class="author-name">Yuechen Zhang</span>, Shengqiong Wu, Xiaohan Wang, Jiebo Luo, Lizi Liao, Hao Fei</div>
          <div class="publication-venue">Preprint, 2025</div>
          <div class="publication-links">
            <a href="https://univa.online/">Project</a> /
            <a href="https://arxiv.org/abs/2511.08521">arXiv</a> /
            <a href="https://github.com/univa-agent/univa" class="code-link" data-github-repo="univa-agent/univa">Code</a><span class="github-stars" data-github-repo="univa-agent/univa"></span>
          </div>
        </div>
      </div>

      <!-- KDiffusion (AAAI 2024) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/kdiffusion.png" alt="KDiffusion" class="pub-img-default" />
          <img src="images/kdiffusion.png" alt="KDiffusion" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">Progressively Knowledge Distillation via Re-parameterizing Diffusion Reverse Process</div>
          <div class="publication-authors">Xufeng Yao, Fanbin Lu, <span class="author-name">Yuechen Zhang</span>, Xinyun Zhang, Wenqian Zhao, Bei Yu</div>
          <div class="publication-venue">AAAI, 2024</div>
          <div class="publication-links">
            <a href="https://www.cse.cuhk.edu.hk/~byu/papers/C199-AAAI2024-KDiffusion.pdf">Paper</a>
          </div>
        </div>
      </div>

      <!-- CodeTalker (CVPR 2023) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/codetalker.png" alt="CodeTalker" class="pub-img-default" />
          <video class="pub-img-hover" muted autoplay loop>
            <source src="images/codetalker_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="publication-content">
          <div class="publication-title">CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior</div>
          <div class="publication-authors">Jinbo Xing, Menghan Xia, <span class="author-name">Yuechen Zhang</span>, Xiaodong Cun, Jue Wang, Tien-Tsin Wong</div>
          <div class="publication-venue">CVPR, 2023</div>
          <div class="publication-links">
            <a href="https://arxiv.org/abs/2301.02379">arXiv</a> /
            <a href="https://doubiiu.github.io/projects/codetalker/">Project</a> /
            <a href="https://github.com/Doubiiu/CodeTalker" class="code-link" data-github-repo="Doubiiu/CodeTalker">Code</a><span class="github-stars" data-github-repo="Doubiiu/CodeTalker"></span>
          </div>
        </div>
      </div>

      <!-- R^2 (Preprint 2022) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/r2-default.png" alt="R^2" class="pub-img-default" />
          <img src="images/r2.png" alt="R^2" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">R<sup>2</sup>Former: Probing Region Relationship in Semantic Segmentation Transformers</div>
          <div class="publication-authors"><span class="author-name">Yuechen Zhang</span>, Tiancheng Shen*, Huaijia Lin, Lu Qi, Eric Lo, Jiaya Jia</div>
          <div class="publication-venue">Preprint, 2022</div>
        </div>
      </div>

      <!-- CRM (CVPR 2022) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/crm-default.png" alt="CRM" class="pub-img-default" />
          <img src="images/crm.png" alt="CRM" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">High Quality Segmentation for Ultra High-resolution Images</div>
          <div class="publication-authors">Tiancheng Shen, <span class="author-name">Yuechen Zhang</span>, Lu Qi, Jason Kuen, Xingyu Xie, Jianlong Wu, Zhe Lin, Jiaya Jia</div>
          <div class="publication-venue">CVPR, 2022</div>
          <div class="publication-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Shen_High_Quality_Segmentation_for_Ultra_High-Resolution_Images_CVPR_2022_paper.pdf">Paper</a> /
            <a href="https://github.com/JIA-Lab-research/Entity/tree/main/High-Quality-Segmention" class="code-link" data-github-repo="JIA-Lab-research/Entity">Code</a><span class="github-stars" data-github-repo="JIA-Lab-research/Entity"></span>
          </div>
        </div>
      </div>

      <!-- PCL (CVPR 2022) -->
      <div class="publication-item">
        <div class="publication-image">
          <img src="images/pcl-default.png" alt="PCL" class="pub-img-default" />
          <img src="images/pcl.png" alt="PCL" class="pub-img-hover" />
        </div>
        <div class="publication-content">
          <div class="publication-title">PCL: Proxy-based Contrastive Learning for Domain Generalization</div>
          <div class="publication-authors">Xufeng Yao, Yang Bai, Xinyun Zhang, <span class="author-name">Yuechen Zhang</span>, Qi Sun, Ran Chen, Ruiyu Li, Bei Yu</div>
          <div class="publication-venue">CVPR, 2022</div>
          <div class="publication-links">
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_PCL_Proxy-Based_Contrastive_Learning_for_Domain_Generalization_CVPR_2022_paper.pdf">Paper</a> /
            <a href="https://github.com/yaoxufeng/PCL-Proxy-based-Contrastive-Learning-for-Domain-Generalization" class="code-link" data-github-repo="yaoxufeng/PCL-Proxy-based-Contrastive-Learning-for-Domain-Generalization">Code</a><span class="github-stars" data-github-repo="yaoxufeng/PCL-Proxy-based-Contrastive-Learning-for-Domain-Generalization"></span>
          </div>
        </div>
      </div>

      </div>
    </div>
  </section>
    <!-- ÊïôËÇ≤‰∏éÂ∑•‰ΩúÁªèÂéÜ Section -->
    <section id="education" class="education-section">
      <div class="section-header">
        <span class="section-sub">EDUCATION & EXPERIENCE</span>
        <h2 class="section-title">Education & Work Experience</h2>
      </div>
      <div class="education-list">
        <div class="edu-block">
          <div class="edu-header">
            <span class="edu-school">The Chinese University of Hong Kong</span>
            <span class="edu-location">Hong Kong</span>
          </div>
          <div class="edu-detail">
            <span class="edu-degree">Doctor of Philosophy, Computer Science.</span>
            <span class="edu-time">Aug 2021 - Dec 2025</span>
          </div>
          <div class="edu-desc">Supervisor: Prof. Jia Jiaya, Prof. Eric Lo</div>
          <div class="edu-detail">
            <span class="edu-degree">Bachelor of Computer Science</span>
            <span class="edu-time">Sep 2016 - Jul 2021</span>
          </div>
          <div class="edu-desc">First Class Honour, ELITE Stream</div>
        </div>
        <div class="edu-block">
          <div class="edu-header">
            <span class="edu-school">Nanyang Technological University</span>
            <span class="edu-location">Singapore</span>
          </div>
          <div class="edu-detail">
            <span class="edu-degree">[Exchange] GEM Trailblazer Exchange Program</span>
            <span class="edu-time">Jan 2019 - May 2019</span>
          </div>
        </div>
        <div class="edu-block">
          <div class="edu-header">
            <span class="edu-school">Tsinghua University</span>
            <span class="edu-location">Beijing, China</span>
          </div>
          <div class="edu-detail">
            <span class="edu-degree">[Exchange] Yao Class Summer Program</span>
            <span class="edu-time">Jul 2019 - Aug 2019</span>
          </div>
        </div>
      </div>
      <div class="work-list">
        <div class="work-block">
          <div class="work-header">
            <span class="work-company">Xiaomi MiMo</span>
            <span class="work-location">Beijing, China</span>
          </div>
          <div class="work-detail">
            <span class="work-title">Research Scientist, Multi-Modal</span>
            <span class="work-time">Jan 2026 - Present</span>
          </div>
          <div class="work-desc">Multi-modal research and development.</div>
        </div>
        <div class="work-block">
          <div class="work-header">
            <span class="work-company">WanX, Alibaba</span>
            <span class="work-location">Hangzhou, China</span>
          </div>
          <div class="work-detail">
            <span class="work-title">Research Internship</span>
            <span class="work-time">Jul 2025 - Sep 2025</span>
          </div>
          <div class="work-desc">A short research intern on lonvideo generation.</div>
        </div>
        <div class="work-block">
          <div class="work-header">
            <span class="work-company">Kling, Kuaishou</span>
            <span class="work-location">Shenzhen, China</span>
          </div>
          <div class="work-detail">
            <span class="work-title">Research Internship</span>
            <span class="work-time">Feb 2025 - June 2025</span>
          </div>
          <div class="work-desc">Research on efficient video generation.</div>
        </div>
        <div class="work-block">
          <div class="work-header">
            <span class="work-company">LightSpeed, Tencent</span>
            <span class="work-location">Hong Kong</span>
          </div>
          <div class="work-detail">
            <span class="work-title">Research Internship</span>
            <span class="work-time">Feb 2024 - Jul 2024</span>
          </div>
          <div class="work-desc">Research on interactive video generation and customization.</div>
        </div>
        <div class="work-block">
          <div class="work-header">
            <span class="work-company">SmartMore</span>
            <span class="work-location">Hong Kong</span>
          </div>
          <div class="work-detail">
            <span class="work-title">Work Study Internship</span>
            <span class="work-time">Jan 2020 - Jul 2025</span>
          </div>
          <div class="work-desc">Research on image segmentation on real-world industry projects, including defect detection and chip circuit high-precision instance segmentation. Developing an algorithm for 2D Datamatrix code recognition and decoding.</div>
        </div>
      </div>
    </section>

    <!-- AWARDS & COMMUNITY CONTRIBUTIONS Section -->
    <section id="awards" class="awards-section">
      <div class="section-header">
        <span class="section-sub">AWARDS & COMMUNITY</span>
        <h2 class="section-title">Awards & Community Contributions</h2>
      </div>
      <ul class="awards-list">
        <li>CUHK ELITE Stream Scholarship, 2016-2017, 2017-2018</li>
        <li>CUHK CSE Academic Outstanding Award, 2017-2018, 2018-2019, 2019-2020</li>
        <li>CUHK Faculty of Engineering, Dean's List, 2016-2017, 2017-2018</li>
        <li>CWChu College Exchange Scholarship, 2018-2019</li>
        <li>CWChu College Scholarships for Academic Excellence, 2017-2018, 2018-2019, 2019-2020</li>
        <li>Teaching Assistant: CSCI3280 Intro. to Multimedia, ESTR4999 Final Year Project, CSCI3250/3251: Computer and Society.</li>
        <li>Conference Reviewer: CVPR, NeurIPS, ICLR, ICML, ICCV, SIGGRAPH, ECCV, AAAI</li>
        <li>Top Reviewer: NeurIPS 2025</li>
      </ul>
    </section>

    <script>
      const headerContainer = document.querySelector('.header-container');
      if (headerContainer) {
        const headerMaskLayer = headerContainer.querySelector('.header-mask-layer');
        const headerPatterns = Array.from(headerContainer.querySelectorAll('.header-pattern'));
        const motionToggle = headerContainer.querySelector('.header-motion-toggle');
        let targetX = 50;
        let targetY = 50;
        let currentX = 50;
        let currentY = 50;
        let trailX = 50;
        let trailY = 50;
        let targetPatternX = 0;
        let targetPatternY = 0;
        let currentPatternX = 0;
        let currentPatternY = 0;
        let rafId = null;
        let isDragging = false;
        let maskRadius = 500;
        let motionEnabled = true;
        let isCircleAnimating = false;

        const isPortrait = () => window.innerHeight > window.innerWidth;

        const updateMaskRadius = () => {
          const raw = getComputedStyle(headerContainer).getPropertyValue('--spot-size');
          const parsed = Number.parseFloat(raw);
          maskRadius = Number.isFinite(parsed) ? parsed : 500;
        };

        const buildCapsulePath = (x1, y1, x2, y2, r) => {
          const dx = x2 - x1;
          const dy = y2 - y1;
          const dist = Math.hypot(dx, dy);
          if (dist < 1) {
            const startX = x1 + r;
            return `M ${startX} ${y1} A ${r} ${r} 0 1 0 ${x1 - r} ${y1} A ${r} ${r} 0 1 0 ${startX} ${y1} Z`;
          }
          const angle = Math.atan2(dy, dx);
          const offsetX = Math.sin(angle) * r;
          const offsetY = -Math.cos(angle) * r;
          const p1aX = x1 + offsetX;
          const p1aY = y1 + offsetY;
          const p1bX = x1 - offsetX;
          const p1bY = y1 - offsetY;
          const p2aX = x2 + offsetX;
          const p2aY = y2 + offsetY;
          const p2bX = x2 - offsetX;
          const p2bY = y2 - offsetY;
          return [
            `M ${p1aX} ${p1aY}`,
            `L ${p2aX} ${p2aY}`,
            `A ${r} ${r} 0 0 1 ${p2bX} ${p2bY}`,
            `L ${p1bX} ${p1bY}`,
            `A ${r} ${r} 0 0 1 ${p1aX} ${p1aY}`,
            'Z'
          ].join(' ');
        };

        const buildPattern = (patternElement) => {
          if (!patternElement) {
            return;
          }
          const rows = 16;
          const cols = 30;
          const text = 'YUECHEN';
          const fragment = document.createDocumentFragment();
          for (let i = 0; i < rows; i += 1) {
            const row = document.createElement('div');
            row.className = 'header-pattern-row';
            for (let j = 0; j < cols; j += 1) {
              const span = document.createElement('span');
              span.className = 'header-pattern-text';
              span.textContent = text;
              row.appendChild(span);
            }
            fragment.appendChild(row);
          }
          patternElement.innerHTML = '';
          patternElement.appendChild(fragment);
        };

        const animateSpotlight = () => {
          if (!motionEnabled) {
            rafId = null;
            return;
          }
          currentX += (targetX - currentX) * 0.18;
          currentY += (targetY - currentY) * 0.18;
          trailX += (currentX - trailX) * 0.08;
          trailY += (currentY - trailY) * 0.08;
          currentPatternX += (targetPatternX - currentPatternX) * 0.12;
          currentPatternY += (targetPatternY - currentPatternY) * 0.12;
          const bounds = headerContainer.getBoundingClientRect();
          const currentPxX = (currentX / 100) * bounds.width;
          const currentPxY = (currentY / 100) * bounds.height;
          const trailPxX = (trailX / 100) * bounds.width;
          const trailPxY = (trailY / 100) * bounds.height;
          if (headerMaskLayer) {
            const path = buildCapsulePath(currentPxX, currentPxY, trailPxX, trailPxY, maskRadius);
            headerMaskLayer.style.clipPath = `path('${path}')`;
            headerMaskLayer.style.webkitClipPath = `path('${path}')`;
          }
          headerPatterns.forEach((patternElement) => {
            patternElement.style.setProperty('--pattern-x', `${currentPatternX}px`);
            patternElement.style.setProperty('--pattern-y', `${currentPatternY}px`);
          });
          rafId = requestAnimationFrame(animateSpotlight);
        };

        const updateTarget = (event) => {
          if (!motionEnabled) {
            return;
          }
          const bounds = headerContainer.getBoundingClientRect();
          targetX = ((event.clientX - bounds.left) / bounds.width) * 100;
          targetY = ((event.clientY - bounds.top) / bounds.height) * 100;
          if (!isDragging) {
            const relativeX = (event.clientX - bounds.left - bounds.width / 2) / bounds.width;
            const relativeY = (event.clientY - bounds.top - bounds.height / 2) / bounds.height;
            targetPatternX = relativeX * 80;
            targetPatternY = relativeY * 60;
          } else {
            targetPatternX += event.movementX;
            targetPatternY += event.movementY;
          }
          headerContainer.classList.add('header-spotlight-active');
          if (!rafId) {
            rafId = requestAnimationFrame(animateSpotlight);
          }
        };

        const stopSpotlight = () => {
          headerContainer.classList.remove('header-spotlight-active');
          if (rafId) {
            cancelAnimationFrame(rafId);
            rafId = null;
          }
        };

        const setMotionEnabled = (enabled, centerPercent) => {
          if (isCircleAnimating) return;
          motionEnabled = enabled;
          if (enabled) {
            headerContainer.classList.remove('header-motion-off');
          }
          if (motionToggle) {
            motionToggle.setAttribute('aria-pressed', String(enabled));
            motionToggle.textContent = enabled ? 'Motion On' : 'Motion Off';
          }
          const bounds = headerContainer.getBoundingClientRect();
          // Close: always use current spotlight position so circle shrinks in place (no teleport)
          const closeX = currentX;
          const closeY = currentY;
          // Open: use click position; will sync spotlight state to this position when animation ends
          const openX = centerPercent ? centerPercent.x : 50;
          const openY = centerPercent ? centerPercent.y : 50;
          if (!enabled) {
            if (headerMaskLayer) {
              isCircleAnimating = true;
              headerMaskLayer.classList.add('header-mask-layer--circle');
              headerMaskLayer.style.opacity = '1';
              headerMaskLayer.style.clipPath = `circle(${maskRadius}px at ${closeX}% ${closeY}%)`;
              headerMaskLayer.style.webkitClipPath = headerMaskLayer.style.clipPath;
              stopSpotlight();
              requestAnimationFrame(() => {
                requestAnimationFrame(() => {
                  headerMaskLayer.style.clipPath = `circle(0 at ${closeX}% ${closeY}%)`;
                  headerMaskLayer.style.webkitClipPath = headerMaskLayer.style.clipPath;
                  const onCloseEnd = () => {
                    headerMaskLayer.removeEventListener('transitionend', onCloseEnd);
                    headerMaskLayer.classList.add('header-mask-layer--closing');
                    headerContainer.classList.add('header-motion-off');
                    headerMaskLayer.style.opacity = '0';
                    requestAnimationFrame(() => {
                      headerMaskLayer.classList.remove('header-mask-layer--circle', 'header-mask-layer--closing');
                      headerMaskLayer.style.opacity = '';
                      isCircleAnimating = false;
                    });
                  };
                  headerMaskLayer.addEventListener('transitionend', onCloseEnd);
                });
              });
            } else {
              stopSpotlight();
              headerContainer.classList.add('header-motion-off');
            }
          } else {
            if (headerMaskLayer) {
              isCircleAnimating = true;
              headerMaskLayer.classList.add('header-mask-layer--circle');
              headerMaskLayer.style.opacity = '1';
              headerMaskLayer.style.clipPath = `circle(0 at ${openX}% ${openY}%)`;
              headerMaskLayer.style.webkitClipPath = headerMaskLayer.style.clipPath;
              requestAnimationFrame(() => {
                headerMaskLayer.style.clipPath = `circle(${maskRadius}px at ${openX}% ${openY}%)`;
                headerMaskLayer.style.webkitClipPath = headerMaskLayer.style.clipPath;
                const onOpenEnd = () => {
                  headerMaskLayer.removeEventListener('transitionend', onOpenEnd);
                  headerMaskLayer.classList.remove('header-mask-layer--circle');
                  headerMaskLayer.style.opacity = '';
                  const cx = (openX / 100) * bounds.width;
                  const cy = (openY / 100) * bounds.height;
                  headerMaskLayer.style.clipPath = `path('${buildCapsulePath(cx, cy, cx, cy, maskRadius)}')`;
                  headerMaskLayer.style.webkitClipPath = headerMaskLayer.style.clipPath;
                  // Sync spotlight state to circle center so no jump when handing off to mouse follow
                  currentX = openX;
                  currentY = openY;
                  targetX = openX;
                  targetY = openY;
                  trailX = openX;
                  trailY = openY;
                  headerContainer.classList.add('header-spotlight-active');
                  if (!rafId) rafId = requestAnimationFrame(animateSpotlight);
                  isCircleAnimating = false;
                };
                headerMaskLayer.addEventListener('transitionend', onOpenEnd);
              });
            }
          }
        };

        const startDrag = () => {
          isDragging = true;
          headerContainer.classList.add('header-pattern-dragging');
        };

        const endDrag = () => {
          isDragging = false;
          headerContainer.classList.remove('header-pattern-dragging');
        };

        updateMaskRadius();
        headerPatterns.forEach((patternElement) => buildPattern(patternElement));
        headerContainer.addEventListener('mousemove', updateTarget);
        headerContainer.addEventListener('mouseenter', updateTarget);
        headerContainer.addEventListener('mouseleave', stopSpotlight);
        headerContainer.addEventListener('mousedown', startDrag);
        window.addEventListener('mouseup', endDrag);
        window.addEventListener('resize', updateMaskRadius);
        if (motionToggle) {
          motionToggle.addEventListener('click', (e) => {
            const bounds = headerContainer.getBoundingClientRect();
            const center = {
              x: ((e.clientX - bounds.left) / bounds.width) * 100,
              y: ((e.clientY - bounds.top) / bounds.height) * 100
            };
            setMotionEnabled(!motionEnabled, center);
          });
        }
        headerContainer.addEventListener('dblclick', (e) => {
          if (e.target.closest('.header-motion-controls')) return;
          const bounds = headerContainer.getBoundingClientRect();
          const center = {
            x: ((e.clientX - bounds.left) / bounds.width) * 100,
            y: ((e.clientY - bounds.top) / bounds.height) * 100
          };
          setMotionEnabled(!motionEnabled, center);
        });
        if (isPortrait()) {
          setMotionEnabled(false);
        }
      }

      (function worksCarousel() {
        const carousel = document.querySelector('.works-carousel');
        if (!carousel) return;
        const stage = carousel.querySelector('.works-carousel-stage');
        const track = carousel.querySelector('.works-carousel-track');
        const cards = Array.from(carousel.querySelectorAll('.works-card'));
        const prevBtn = carousel.querySelector('.works-carousel-prev');
        const nextBtn = carousel.querySelector('.works-carousel-next');
        const dotsEl = carousel.querySelector('.works-carousel-dots');
        const total = cards.length;
        let current = 0;
        const gap = 16;
        const isPortrait = () => window.innerHeight > window.innerWidth;
        let dragStartX = 0;
        let dragStartOffset = 0;
        let isDragging = false;
        let dragged = false;
        let justDragged = false;

        // Fixed card widths ‚Äî never read from DOM to avoid transition timing issues
        // On mobile (portrait), abstract is hidden, so expanded = normal width
        const isMobile = window.innerWidth <= 768;
        const CARD_W = isMobile ? Math.min(window.innerWidth * 0.8, 340) : 480;
        const CARD_EXPANDED_W = isMobile ? CARD_W : 1060;

        function getCardWidth() {
          return CARD_W;
        }

        function circDist(i) {
          let d = ((i - current) % total + total) % total;
          if (d > total / 2) d -= total;
          return d;
        }

        function getVisualOrder() {
          const half = Math.floor(total / 2);
          const order = [];
          for (let offset = -half; offset < total - half; offset++) {
            order.push(((current + offset) % total + total) % total);
          }
          return order;
        }

        function applyVisualOrder() {
          const visualOrder = getVisualOrder();
          visualOrder.forEach((cardIdx, pos) => {
            cards[cardIdx].style.order = pos;
          });
        }

        // Use fixed widths: current card = expanded, others = collapsed
        function getBaseOffsetPx() {
          const containerWidth = stage.offsetWidth;
          if (!containerWidth) return 0;
          const visualOrder = getVisualOrder();
          let leftEdge = 0;
          let currentLeft = 0;
          let currentWidth = 0;
          for (let pos = 0; pos < visualOrder.length; pos++) {
            const idx = visualOrder[pos];
            const w = (idx === current) ? CARD_EXPANDED_W : CARD_W;
            if (idx === current) {
              currentLeft = leftEdge;
              currentWidth = w;
            }
            leftEdge += w + gap;
          }
          return containerWidth / 2 - (currentLeft + currentWidth / 2);
        }

        function applyTrackTransform(offsetPx) {
          track.style.transform = `translateX(${offsetPx}px)`;
        }

        function getCardClass(i) {
          const d = circDist(i);
          if (d === 0) return 'works-card--center';
          if (d === -1) return 'works-card--left-1';
          if (d === 1) return 'works-card--right-1';
          if (d === -2) return 'works-card--left-2';
          if (d === 2) return 'works-card--right-2';
          if (d < 0) return 'works-card--far';
          return 'works-card--far works-card--right-far';
        }

        function syncExpandToCurrent() {
          cards.forEach((c) => c.classList.remove('is-expanded'));
          if (cards[current]) cards[current].classList.add('is-expanded');
          onExpandChange();
        }

        function updateCardClasses() {
          cards.forEach((card, i) => {
            card.classList.remove('works-card--center', 'works-card--left-1', 'works-card--right-1', 'works-card--left-2', 'works-card--right-2', 'works-card--far', 'works-card--right-far');
            const cls = getCardClass(i);
            cls.split(' ').forEach(c => { if (c) card.classList.add(c); });
          });
          dotsEl.querySelectorAll('.works-carousel-dot').forEach((dot, i) => {
            dot.classList.toggle('is-active', i === current);
            dot.setAttribute('aria-selected', i === current);
          });
        }

        // Calculate offset to center a specific card, with a custom expanded card
        function calcOffset(centerIdx, expandedIdx) {
          const containerWidth = stage.offsetWidth;
          if (!containerWidth) return 0;
          const visualOrder = getVisualOrder();
          let leftEdge = 0;
          let targetLeft = 0;
          let targetWidth = 0;
          for (let pos = 0; pos < visualOrder.length; pos++) {
            const idx = visualOrder[pos];
            const w = (idx === expandedIdx) ? CARD_EXPANDED_W : CARD_W;
            if (idx === centerIdx) {
              targetLeft = leftEdge;
              targetWidth = w;
            }
            leftEdge += w + gap;
          }
          return containerWidth / 2 - (targetLeft + targetWidth / 2);
        }

        function updateCarousel() {
          if (isPortrait()) {
            track.style.transform = 'none';
            cards.forEach((c) => {
              c.classList.remove('works-card--center', 'works-card--left-1', 'works-card--right-1', 'works-card--left-2', 'works-card--right-2', 'works-card--far', 'works-card--right-far', 'is-expanded');
            });
            dotsEl.querySelectorAll('.works-carousel-dot').forEach((dot, i) => {
              dot.classList.toggle('is-active', i === current);
              dot.setAttribute('aria-selected', i === current);
            });
            onExpandChange();
            return;
          }
          syncExpandToCurrent();
          applyVisualOrder();
          applyTrackTransform(getBaseOffsetPx());
          updateCardClasses();
        }

        function go(step) {
          if (isPortrait()) return;
          const oldCurrent = current;
          current = (current + step + total) % total;

          // 1. Apply new state
          syncExpandToCurrent();
          applyVisualOrder();

          // 2. Snap: center OLD card using its pre-transition width (still visually expanded)
          track.classList.add('is-dragging');
          void track.offsetHeight;
          applyTrackTransform(calcOffset(oldCurrent, oldCurrent));
          void track.offsetHeight;

          // 3. Slide: transition to NEW card centered at its final expanded width
          track.classList.remove('is-dragging');
          applyTrackTransform(getBaseOffsetPx());
          updateCardClasses();
        }

        function buildDots() {
          dotsEl.innerHTML = '';
          for (let i = 0; i < total; i += 1) {
            const btn = document.createElement('button');
            btn.type = 'button';
            btn.className = 'works-carousel-dot' + (i === current ? ' is-active' : '');
            btn.setAttribute('role', 'tab');
            btn.setAttribute('aria-label', 'Slide ' + (i + 1));
            btn.setAttribute('aria-selected', i === current);
            btn.addEventListener('click', () => { current = i; updateCarousel(); });
            dotsEl.appendChild(btn);
          }
        }

        prevBtn.addEventListener('click', () => go(-1));
        nextBtn.addEventListener('click', () => go(1));

        stage.addEventListener('mousedown', (e) => {
          if (isPortrait()) return;
          if (e.target.closest('a') || e.target.closest('button')) return;
          isDragging = true;
          dragged = false;
          dragStartX = e.clientX;
          dragStartOffset = getBaseOffsetPx();
          track.classList.add('is-dragging');
        });

        window.addEventListener('mousemove', (e) => {
          if (!isDragging) return;
          const dx = e.clientX - dragStartX;
          if (Math.abs(dx) > 8) dragged = true;
          applyTrackTransform(dragStartOffset + dx);
        });

        window.addEventListener('mouseup', (e) => {
          if (!isDragging) return;
          isDragging = false;
          track.classList.remove('is-dragging');
          const dx = e.clientX - dragStartX;
          const cardWidth = getCardWidth();
          const threshold = cardWidth * 0.2;
          if (dragged && dx > threshold) go(-1);
          else if (dragged && dx < -threshold) go(1);
          else updateCarousel();
          if (dragged) {
            justDragged = true;
            setTimeout(() => { justDragged = false; }, 100);
          }
        });

        stage.addEventListener('touchstart', (e) => {
          if (isPortrait()) return;
          if (e.target.closest('a') || e.target.closest('button')) return;
          isDragging = true;
          dragged = false;
          dragStartX = e.touches[0].clientX;
          dragStartOffset = getBaseOffsetPx();
          track.classList.add('is-dragging');
        }, { passive: true });

        window.addEventListener('touchmove', (e) => {
          if (!isDragging || !e.touches.length) return;
          const dx = e.touches[0].clientX - dragStartX;
          if (Math.abs(dx) > 8) dragged = true;
          applyTrackTransform(dragStartOffset + dx);
        }, { passive: true });

        window.addEventListener('touchend', (e) => {
          if (!isDragging) return;
          isDragging = false;
          track.classList.remove('is-dragging');
          const touch = e.changedTouches[0];
          if (!touch) return;
          const dx = touch.clientX - dragStartX;
          const cardWidth = getCardWidth();
          const threshold = cardWidth * 0.2;
          if (dragged && dx > threshold) go(-1);
          else if (dragged && dx < -threshold) go(1);
          else updateCarousel();
          if (dragged) {
            justDragged = true;
            setTimeout(() => { justDragged = false; }, 100);
          }
        });

        function collapseAll() {
          cards.forEach((c) => c.classList.remove('is-expanded'));
        }

        function onExpandChange() {
          const expanded = carousel.querySelector('.works-card.is-expanded');
          if (expanded) {
            document.addEventListener('mousedown', handleClickOutside);
          } else {
            document.removeEventListener('mousedown', handleClickOutside);
          }
        }

        function handleClickOutside(e) {
          const expanded = carousel.querySelector('.works-card.is-expanded');
          if (!expanded || expanded.contains(e.target)) return;
          expanded.classList.remove('is-expanded');
          onExpandChange();
          applyTrackTransform(getBaseOffsetPx());
        }

        cards.forEach((card) => {
          const inner = card.querySelector('.works-card-inner');
          const abstractEl = card.querySelector('.works-card-abstract');
          const abstract = card.getAttribute('data-abstract') || '[Abstract placeholder]';
          const closeBtn = document.createElement('button');
          closeBtn.type = 'button';
          closeBtn.className = 'works-card-abstract-close';
          closeBtn.setAttribute('aria-label', 'Close');
          closeBtn.textContent = '√ó';
          const text = document.createElement('p');
          text.className = 'works-card-abstract-text';
          text.textContent = abstract;
          abstractEl.appendChild(closeBtn);
          abstractEl.appendChild(text);
          inner.addEventListener('click', (e) => {
            if (isPortrait()) return;
            if (e.target.closest('a')) return;
            if (justDragged) return;
            const idx = parseInt(card.getAttribute('data-index'), 10);
            if (idx !== current) {
              current = idx;
              updateCarousel();
            }
          });
          closeBtn.addEventListener('click', (e) => {
            e.stopPropagation();
            card.classList.remove('is-expanded');
            onExpandChange();
            applyTrackTransform(getBaseOffsetPx());
          });
        });

        buildDots();
        updateCarousel();
        window.addEventListener('resize', updateCarousel);
        window.addEventListener('orientationchange', () => { setTimeout(updateCarousel, 100); });
      })();

      // Ê†ºÂºèÂåñstarÊï∞ÈáèÔºà‰æãÂ¶ÇÔºö1234 -> "1.2k"Ôºâ
      function formatStars(count) {
        if (count >= 1000) {
          return (count / 1000).toFixed(1) + 'k';
        }
        return count.toString();
      }

      // Ëé∑ÂèñGitHub starÊï∞Èáè
      async function fetchGitHubStars(repo) {
        try {
          const response = await fetch(`https://api.github.com/repos/${repo}`, {
            headers: {
              'Accept': 'application/vnd.github.v3+json'
            }
          });
          if (response.ok) {
            const data = await response.json();
            return data.stargazers_count;
          } else if (response.status === 404) {
            console.warn(`Repository ${repo} not found`);
          } else if (response.status === 403) {
            console.warn(`Rate limit exceeded for ${repo}`);
          } else {
            console.warn(`Failed to fetch ${repo}: ${response.status}`);
          }
        } catch (error) {
          console.error(`Error fetching stars for ${repo}:`, error);
        }
        return null;
      }

      // Êõ¥Êñ∞ÊâÄÊúâGitHub starÊòæÁ§∫
      async function updateAllStars() {
        const starElements = document.querySelectorAll('.github-stars[data-github-repo]');
        if (starElements.length === 0) {
          console.log('No star elements found');
          return;
        }

        console.log(`Found ${starElements.length} star elements`);
        const repos = new Set();
        
        // Êî∂ÈõÜÊâÄÊúâÂîØ‰∏ÄÁöÑ‰ªìÂ∫ì
        starElements.forEach(el => {
          const repo = el.getAttribute('data-github-repo');
          if (repo) {
            repos.add(repo);
            console.log(`Found repo: ${repo}`);
          }
        });

        console.log(`Fetching stars for ${repos.size} unique repos`);
        
        // ÊâπÈáèËé∑ÂèñstarÊï∞ÈáèÔºàÊ∑ªÂä†Âª∂Ëøü‰ª•ÈÅøÂÖçÈÄüÁéáÈôêÂà∂Ôºâ
        const results = [];
        for (const repo of repos) {
          const stars = await fetchGitHubStars(repo);
          results.push({ repo, stars });
          console.log(`Repo ${repo}: ${stars !== null ? stars : 'failed'} stars`);
          // Ê∑ªÂä†Â∞èÂª∂Ëøü‰ª•ÈÅøÂÖçËß¶ÂèëGitHub APIÈÄüÁéáÈôêÂà∂
          await new Promise(resolve => setTimeout(resolve, 200));
        }
        
        // Êõ¥Êñ∞ÊâÄÊúâÂØπÂ∫îÁöÑÂÖÉÁ¥†
        let updatedCount = 0;
        starElements.forEach(el => {
          const repo = el.getAttribute('data-github-repo');
          const result = results.find(r => r.repo === repo);
          if (result && result.stars !== null && result.stars !== undefined) {
            el.textContent = ` ‚≠ê ${formatStars(result.stars)}`;
            el.style.display = 'inline';
            updatedCount++;
          } else {
            el.style.display = 'none';
          }
        });
        
        console.log(`Updated ${updatedCount} star displays`);
      }

      // È°µÈù¢Âä†ËΩΩÂÆåÊàêÂêéËé∑ÂèñstarÊï∞Èáè
      if (document.readyState === 'loading') {
        document.addEventListener('DOMContentLoaded', updateAllStars);
      } else {
        // Âª∂Ëøü‰∏ÄÁÇπÊâßË°åÔºåÁ°Æ‰øùDOMÂÆåÂÖ®Âä†ËΩΩ
        setTimeout(updateAllStars, 100);
      }

      // Portrait: toggle image on click (Representative works + Full pub)
      (function () {
        var portraitQuery = window.matchMedia('(orientation: portrait)');
        function isPortrait() {
          return portraitQuery.matches;
        }
        function onImageClick(e) {
          if (!isPortrait()) return;
          e.preventDefault();
          this.classList.toggle('img-revealed');
        }
        function clearRevealed() {
          document.querySelectorAll('.bigcard-img-wrap.img-revealed, .publication-image.img-revealed').forEach(function (el) {
            el.classList.remove('img-revealed');
          });
        }
        function init() {
          document.querySelectorAll('.bigcard-img-wrap').forEach(function (el) {
            el.addEventListener('click', onImageClick);
          });
          document.querySelectorAll('.publication-image').forEach(function (el) {
            el.addEventListener('click', onImageClick);
          });
          portraitQuery.addEventListener('change', function () {
            if (!portraitQuery.matches) clearRevealed();
          });
        }
        if (document.readyState === 'loading') {
          document.addEventListener('DOMContentLoaded', init);
        } else {
          init();
        }
      })();
     </script>
</body>
</html>
